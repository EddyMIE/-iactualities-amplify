# üîÑ Guide de Conversion React/TSX ‚Üí Streamlit - IA'ctualit√©s

## üö® Probl√®me Identifi√©

Votre application actuelle utilise :
- **Frontend** : React + TypeScript (TSX)
- **Backend** : Python FastAPI

**Streamlit ne supporte que Python**, donc il faut convertir l'interface React en Streamlit.

## üîÑ Plan de Conversion

### **√âtape 1 : Analyser les Composants React**

Votre application React contient :
```tsx
// Composants √† convertir
- ModelSelector (s√©lection des mod√®les)
- QuestionInput (saisie de question)
- ResultsDisplay (affichage des r√©sultats)
- CostSummary (r√©sum√© des co√ªts)
- RobotAssistant (assistant visuel)
```

### **√âtape 2 : Conversion vers Streamlit**

```python
# streamlit_app.py - Version compl√®te
import streamlit as st
import requests
import json
from typing import List, Dict
import time

# Configuration de la page
st.set_page_config(
    page_title="IA'ctualit√©s - Comparateur de Mod√®les LLM",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√© pour reproduire le design React
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #1a237e 0%, #3949ab 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .model-card {
        border: 2px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        transition: all 0.3s ease;
    }
    
    .model-card.selected {
        border-color: #1a237e;
        background-color: #f3f4f6;
    }
    
    .result-card {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 1.5rem;
        margin: 1rem 0;
        background-color: white;
    }
    
    .cost-badge {
        background-color: #4caf50;
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 4px;
        font-size: 0.8rem;
    }
    
    .robot-assistant {
        text-align: center;
        padding: 2rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 15px;
        color: white;
        margin: 2rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Variables d'environnement
BEDROCK_ENDPOINT = st.secrets.get("BEDROCK_ENDPOINT", "http://localhost:8000")
AZURE_OPENAI_KEY = st.secrets.get("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = st.secrets.get("AZURE_OPENAI_ENDPOINT")

# Mod√®les disponibles (identique √† votre React)
AVAILABLE_MODELS = [
    {"name": "Claude 3 Sonnet", "provider": "bedrock", "cost_per_1k": 0.003, "description": "Mod√®le √©quilibr√© pour la plupart des t√¢ches"},
    {"name": "Claude 3 Haiku", "provider": "bedrock", "cost_per_1k": 0.00025, "description": "Rapide et √©conomique"},
    {"name": "Claude 3.7 Sonnet", "provider": "bedrock", "cost_per_1k": 0.003, "description": "Version am√©lior√©e de Claude 3 Sonnet"},
    {"name": "GPT-4o (Azure)", "provider": "azure", "cost_per_1k": 0.005, "description": "Mod√®le OpenAI via Azure"},
    {"name": "GPT-4o Mini (Azure)", "provider": "azure", "cost_per_1k": 0.00015, "description": "Version compacte et rapide"},
    {"name": "Mixtral 8x7B Instruct", "provider": "bedrock", "cost_per_1k": 0.00024, "description": "Mod√®le open source performant"},
    {"name": "Mistra 8x7B Instruct", "provider": "bedrock", "cost_per_1k": 0.00014, "description": "Mod√®le Mistral AI"},
    {"name": "Pixtral Large", "provider": "bedrock", "cost_per_1k": 0.00024, "description": "Mod√®le Pixtral optimis√©"}
]

def query_model(model_name: str, prompt: str) -> Dict:
    """Interroge un mod√®le LLM (identique √† votre backend)"""
    try:
        # Utilise votre backend existant
        response = requests.post(f"{BEDROCK_ENDPOINT}/query", 
                               json={"model": model_name, "prompt": prompt},
                               timeout=60)
        
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Erreur {response.status_code}: {response.text}"}
    except Exception as e:
        return {"error": f"Erreur de connexion: {str(e)}"}

def format_response(text: str) -> str:
    """Formate la r√©ponse comme dans votre React (simplifi√©)"""
    # Conversion basique markdown ‚Üí HTML
    formatted = text.replace("**", "<strong>").replace("**", "</strong>")
    formatted = formatted.replace("*", "<em>").replace("*", "</em>")
    formatted = formatted.replace("\n\n", "<br><br>")
    return formatted

def main():
    # Header principal (√©quivalent √† votre React)
    st.markdown("""
    <div class="main-header">
        <h1>ü§ñ IA'ctualit√©s</h1>
        <p>Comparateur de Mod√®les LLM - Analysez et comparez les r√©ponses d'intelligence artificielle</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Robot Assistant (√©quivalent √† votre composant React)
    st.markdown("""
    <div class="robot-assistant">
        <h2>ü§ñ Assistant IA'ctualit√©s</h2>
        <p>Je suis l√† pour vous aider √† comparer les mod√®les d'IA !</p>
        <p>üí° <strong>Conseil</strong> : S√©lectionnez 2-3 mod√®les pour une comparaison optimale</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Layout en colonnes (√©quivalent √† votre React)
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("### üéØ S√©lection des Mod√®les")
        st.info("S√©lectionnez jusqu'√† **3 mod√®les maximum** pour √©viter les erreurs de serveur")
        
        # S√©lection des mod√®les (√©quivalent √† ModelSelector)
        selected_models = []
        for i, model in enumerate(AVAILABLE_MODELS):
            # Cr√©er une cl√© unique pour chaque checkbox
            key = f"model_{i}_{model['name'].replace(' ', '_')}"
            
            if st.checkbox(f"**{model['name']}** ({model['provider']})", key=key):
                if len(selected_models) < 3:
                    selected_models.append(model['name'])
                    st.success(f"‚úÖ {model['name']} ajout√©")
                else:
                    st.warning("‚ö†Ô∏è Limite de 3 mod√®les atteinte")
                    # D√©cocher automatiquement
                    st.session_state[key] = False
                    break
        
        # Compteur de mod√®les s√©lectionn√©s
        st.metric("Mod√®les s√©lectionn√©s", f"{len(selected_models)}/3")
        
        # Message d'encouragement (√©quivalent √† votre React)
        if len(selected_models) == 0:
            st.info("üí° S√©lectionnez au moins un mod√®le pour commencer")
        elif len(selected_models) == 1:
            st.success("üéØ Comparaison cibl√©e ! Vous pouvez ajouter 2 mod√®les suppl√©mentaires")
        elif len(selected_models) == 2:
            st.success("üöÄ Excellente s√©lection ! Vous pouvez encore ajouter 1 mod√®le")
        else:
            st.success("üéâ Parfait ! 3 mod√®les offrent un bon √©quilibre")
    
    with col2:
        st.markdown("### üí≠ Posez votre Question")
        
        # Zone de saisie (√©quivalent √† QuestionInput)
        question = st.text_area(
            "Votre question :",
            placeholder="Ex: Expliquez la th√©orie de la relativit√© en termes simples...",
            height=120,
            help="Saisissez votre question ici. Elle sera envoy√©e √† tous les mod√®les s√©lectionn√©s."
        )
        
        # Bouton de comparaison (√©quivalent √† votre React)
        if st.button("üöÄ Comparer les Mod√®les", type="primary", disabled=len(selected_models) == 0):
            if not question.strip():
                st.error("‚ùå Veuillez saisir une question")
                return
            
            # Affichage des r√©sultats (√©quivalent √† ResultsDisplay)
            with st.spinner("üîÑ Analyse en cours..."):
                results = []
                total_cost = 0
                
                # Traitement s√©quentiel pour √©viter la surcharge
                for i, model_name in enumerate(selected_models):
                    st.markdown(f"### üìä {model_name}")
                    
                    with st.expander(f"R√©sultats d√©taill√©s - {model_name}", expanded=True):
                        # Appel au mod√®le
                        result = query_model(model_name, question)
                        
                        if "error" in result:
                            st.error(f"‚ùå Erreur: {result['error']}")
                        else:
                            # Affichage de la r√©ponse format√©e
                            st.markdown("#### R√©ponse :")
                            formatted_response = format_response(result.get("response", ""))
                            st.markdown(formatted_response, unsafe_allow_html=True)
                            
                            # M√©triques (√©quivalent √† CostSummary)
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                cost = result.get("cost", 0)
                                st.metric("üí∞ Co√ªt", f"${cost:.4f}")
                                total_cost += cost
                            with col2:
                                st.metric("üìù Tokens", result.get("tokens", 0))
                            with col3:
                                st.metric("‚ö° Mod√®le", model_name)
                            
                            # Barre de progression
                            st.progress((i + 1) / len(selected_models))
                    
                    # D√©lai entre les requ√™tes pour √©viter la surcharge
                    if i < len(selected_models) - 1:
                        time.sleep(0.5)
                
                # R√©sum√© final (√©quivalent √† CostSummary)
                st.markdown("---")
                st.markdown("### üìà R√©sum√© de la Comparaison")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("üéØ Mod√®les test√©s", len(selected_models))
                with col2:
                    st.metric("üí∞ Co√ªt total", f"${total_cost:.4f}")
                with col3:
                    st.metric("‚ö° Temps moyen", "~2-5s par mod√®le")
                
                # Recommandations
                st.markdown("### üí° Recommandations")
                if total_cost < 0.01:
                    st.success("‚úÖ Co√ªt tr√®s √©conomique !")
                elif total_cost < 0.05:
                    st.info("üí∞ Co√ªt mod√©r√©")
                else:
                    st.warning("‚ö†Ô∏è Co√ªt √©lev√©, consid√©rez moins de mod√®les")
    
    # Footer (√©quivalent √† votre React)
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 2rem;'>
        <p><strong>ü§ñ IA'ctualit√©s - Comparateur de Mod√®les LLM</strong></p>
        <p>Propuls√© par AWS Bedrock et Azure OpenAI</p>
        <p>üí° Optimis√© pour √©viter les erreurs de serveur</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
```

### **√âtape 3 : Configuration des D√©pendances**

```txt
# requirements.txt
streamlit>=1.28.0
requests>=2.31.0
boto3>=1.34.0
openai>=1.3.0
python-dotenv>=1.0.0
pandas>=2.0.0
plotly>=5.17.0
```

### **√âtape 4 : Variables d'Environnement Streamlit**

```bash
# Dans Streamlit Cloud, configurez :
BEDROCK_ENDPOINT=https://your-backend.railway.app
AZURE_OPENAI_KEY=votre_cl√©_azure
AZURE_OPENAI_ENDPOINT=votre_endpoint_azure
```

## üéØ **Avantages de cette Conversion**

### ‚úÖ **Avantages :**
- **D√©ploiement ultra-rapide** (5 minutes)
- **Gratuit** pour commencer
- **Pas de configuration complexe**
- **Int√©gration Python native**
- **M√™me fonctionnalit√©s** que votre React

### ‚ö†Ô∏è **Limitations :**
- **Interface moins moderne** que React
- **Moins de personnalisation** CSS
- **Pas d'animations complexes**
- **Performance limit√©e** pour les gros volumes

## üîÑ **Alternative : Garder React + Backend Python**

Si vous voulez garder votre interface React, utilisez plut√¥t :

### **Option 2 : Vercel + Railway (Recommand√©e)**
- **Frontend** : Vercel (React/TSX)
- **Backend** : Railway (Python FastAPI)
- **Avantages** : Garde votre code React, d√©ploiement moderne

### **Option 3 : AWS Amplify + Lambda**
- **Frontend** : Amplify (React/TSX)
- **Backend** : Lambda (Python)
- **Avantages** : Scalabilit√© maximale, int√©gration AWS

## üéØ **Ma Recommandation**

**Pour votre cas sp√©cifique :**

1. **Si vous voulez d√©ployer rapidement** ‚Üí **Streamlit** (conversion n√©cessaire)
2. **Si vous voulez garder React** ‚Üí **Vercel + Railway** (pas de conversion)
3. **Si vous voulez la scalabilit√©** ‚Üí **AWS Amplify + Lambda** (pas de conversion)

**La conversion Streamlit prendra environ 2-3 heures** pour reproduire toutes vos fonctionnalit√©s React. 