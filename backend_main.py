#!/usr/bin/env python3
"""
Backend IA'ctualit√©s avec vraies API (AWS Bedrock + Azure OpenAI)
Version adapt√©e pour l'interface React moderne
"""

import os
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import boto3
from pydantic import BaseModel
import json
import time
from openai import AzureOpenAI

load_dotenv()

app = FastAPI(title="IA'ctualit√©s Real Backend", version="1.0.0")

# Configuration CORS pour React
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration des mod√®les AWS Bedrock
BEDROCK_MODELS = {
    "Mixtral 8x7B Instruct": "mistral.mixtral-8x7b-instruct-v0:1",
    "Claude 3 Sonnet": "eu.anthropic.claude-3-sonnet-20240229-v1:0", 
    "Claude 3 Haiku": "eu.anthropic.claude-3-haiku-20240307-v1:0",
    "Claude 3.7 Sonnet": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
    "Pixtral Large": "eu.mistral.pixtral-large-2502-v1:0"
}

# Configuration des mod√®les avec capacit√©s internet
INTERNET_ENABLED_MODELS = {
    "Claude 3.7 Sonnet": True,
    "Claude 3 Sonnet": False,
    "Claude 3 Haiku": False,
    "Mixtral 8x7B Instruct": False,
    "Pixtral Large": False
}

# Configuration Azure OpenAI avec vos d√©ploiements
AZURE_MODELS = {
    "GPT-4o (Azure)": {
        "deployment": "Eddy-deploy-20-02-2025-gpt-4o",
        "api_version": "2025-01-01-preview",
        "api_key_var": "GPT4O_API_KEY",
        "endpoint_var": "GPT4O_ENDPOINT"
    },
    "GPT-4o Mini (Azure)": {
        "deployment": "Eddy-02-2025-gpt-4o-mini", 
        "api_version": "2024-12-01-preview",
        "api_key_var": "GPT4O_MINI_API_KEY",
        "endpoint_var": "GPT4O_MINI_ENDPOINT"
    }
}

# Co√ªts par mod√®le (USD par 1000 tokens)
MODEL_COSTS = {
    "Mixtral 8x7B Instruct": {"input": 0.0007, "output": 0.0007},
    "Claude 3 Sonnet": {"input": 0.003, "output": 0.015},
    "Claude 3 Haiku": {"input": 0.00025, "output": 0.00125},
    "Claude 3.7 Sonnet": {"input": 0.003, "output": 0.015},
    "Pixtral Large": {"input": 0.002, "output": 0.006},
    "GPT-4o (Azure)": {"input": 0.005, "output": 0.015},
    "GPT-4o Mini (Azure)": {"input": 0.00015, "output": 0.0006}
}

class QueryRequest(BaseModel):
    model: str
    prompt: str

class QueryResponse(BaseModel):
    response: str
    tokens: int
    cost: float

def get_bedrock_client():
    """Initialise le client AWS Bedrock"""
    return boto3.client(
        "bedrock-runtime",
        region_name=os.getenv("CUSTOM_AWS_REGION", "eu-west-3"),
        aws_access_key_id=os.getenv("CUSTOM_AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=os.getenv("CUSTOM_AWS_SECRET_ACCESS_KEY")
    )

def get_azure_client(model_name: str):
    """Initialise le client Azure OpenAI pour un mod√®le sp√©cifique"""
    model_config = AZURE_MODELS[model_name]
    return AzureOpenAI(
        api_key=os.getenv(model_config["api_key_var"]),
        api_version=model_config["api_version"],
        azure_endpoint=os.getenv(model_config["endpoint_var"])
    )

def estimate_tokens(text: str) -> int:
    """Estimation approximative du nombre de tokens"""
    return max(1, len(text.split()) * 1.3)

def calculate_cost(model_name: str, input_text: str, output_text: str) -> float:
    """Calcule le co√ªt bas√© sur les tokens d'entr√©e et sortie"""
    if model_name not in MODEL_COSTS:
        return 0.0
    
    input_tokens = estimate_tokens(input_text)
    output_tokens = estimate_tokens(output_text)
    
    costs = MODEL_COSTS[model_name]
    total_cost = (input_tokens * costs["input"] / 1000) + (output_tokens * costs["output"] / 1000)
    
    return round(total_cost, 6)

def query_bedrock_model(model_name: str, prompt: str) -> dict:
    """Interroge un mod√®le AWS Bedrock"""
    try:
        bedrock = get_bedrock_client()
        model_id = BEDROCK_MODELS[model_name]
        
        # Instructions optimis√©es avec formatage structur√©
        prompt_with_instruction = f"""Tu es un assistant IA sp√©cialis√© en conseil et gestion de projet.

Question : {prompt}

Instructions :
- R√©ponds de mani√®re professionnelle et structur√©e
- Utilise des titres et sous-titres pour organiser ta r√©ponse (## Titre principal, ### Sous-titre)
- Utilise des listes √† puces (- ou *) pour les √©tapes, avantages, inconv√©nients
- Utilise des listes num√©rot√©es (1. 2. 3.) pour les processus s√©quentiels
- Mets en gras les points importants avec **texte**
- Utilise des exemples concrets quand c'est pertinent
- Sois pr√©cis et pratique dans tes conseils
- Limite ta r√©ponse √† 500 mots maximum

R√©ponse :"""
        
        # Construction du body selon le mod√®le
        if model_id.startswith("eu.anthropic.claude-3"):
            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "temperature": 0.3,
                "messages": [
                    {"role": "user", "content": [{"type": "text", "text": prompt_with_instruction}]}
                ]
            })
        elif model_id.startswith("mistral.") or model_id.startswith("eu.mistral."):
            body = json.dumps({
                "prompt": f"<s>[INST] {prompt_with_instruction} [/INST]",
                "max_tokens": 1000,
                "temperature": 0.3
            })
        else:
            raise HTTPException(status_code=400, detail=f"Mod√®le non support√© : {model_id}")
        
        # Appel √† l'API
        response = bedrock.invoke_model(
            modelId=model_id,
            body=body,
            accept="application/json",
            contentType="application/json"
        )
        
        raw_response = response["body"].read().decode("utf-8")
        
        # Extraction de la r√©ponse
        try:
            response_data = json.loads(raw_response)
            if model_id.startswith("eu.anthropic.claude-3"):
                if "content" in response_data and isinstance(response_data["content"], list):
                    text = "".join([c.get("text", "") for c in response_data["content"]])
                else:
                    text = raw_response
            elif model_id.startswith("mistral.") or model_id.startswith("eu.mistral."):
                if "outputs" in response_data:
                    text = response_data["outputs"][0]["text"]
                elif "choices" in response_data and len(response_data["choices"]) > 0:
                    text = response_data["choices"][0]["message"]["content"]
                else:
                    text = raw_response
            else:
                text = raw_response
        except json.JSONDecodeError:
            text = raw_response
        
        # Calcul des m√©triques
        tokens = estimate_tokens(prompt) + estimate_tokens(text)
        cost = calculate_cost(model_name, prompt, text)
        
        return {
            "response": text.strip(),
            "tokens": int(tokens),
            "cost": cost
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur Bedrock pour {model_name}: {str(e)}")

def query_azure_model(model_name: str, prompt: str) -> dict:
    """Interroge un mod√®le Azure OpenAI"""
    try:
        client = get_azure_client(model_name)
        model_config = AZURE_MODELS[model_name]
        deployment_name = model_config["deployment"]
        
        # Message syst√®me avec formatage structur√©
        system_message = """Tu es un assistant IA sp√©cialis√© en conseil et gestion de projet.

Instructions :
- R√©ponds de mani√®re professionnelle et structur√©e
- Utilise des titres et sous-titres pour organiser ta r√©ponse (## Titre principal, ### Sous-titre)
- Utilise des listes √† puces (- ou *) pour les √©tapes, avantages, inconv√©nients
- Utilise des listes num√©rot√©es (1. 2. 3.) pour les processus s√©quentiels
- Mets en gras les points importants avec **texte**
- Utilise des exemples concrets quand c'est pertinent
- Sois pr√©cis et pratique dans tes conseils
- Limite ta r√©ponse √† 500 mots maximum"""
        
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        text = response.choices[0].message.content or ""
        
        # Calcul des m√©triques
        tokens = estimate_tokens(prompt) + estimate_tokens(text)
        cost = calculate_cost(model_name, prompt, text)
        
        return {
            "response": text.strip(),
            "tokens": int(tokens), 
            "cost": cost
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur Azure pour {model_name}: {str(e)}")

def analyze_prompt_type(prompt: str) -> str:
    """Analyse le type de prompt pour appliquer les bonnes techniques"""
    prompt_lower = prompt.lower()
    
    # D√©tection du type de question
    if any(word in prompt_lower for word in ['comparer', 'diff√©rence', 'vs', 'versus', 'contre']):
        return "comparison"
    elif any(word in prompt_lower for word in ['analyser', 'analyse', 'expliquer', 'pourquoi']):
        return "analysis"
    elif any(word in prompt_lower for word in ['cr√©er', 'g√©n√©rer', '√©crire', 'r√©diger', 'inventer']):
        return "creation"
    elif any(word in prompt_lower for word in ['rechercher', 'trouver', 'quand', 'o√π', 'qui']):
        return "research"
    elif any(word in prompt_lower for word in ['calculer', 'r√©soudre', 'probl√®me', '√©quation']):
        return "calculation"
    else:
        return "general"

def get_prompt_template(prompt_type: str, original_prompt: str) -> str:
    """Retourne le template d'am√©lioration selon le type de prompt"""
    
    templates = {
        "comparison": f"""Am√©liore ce prompt de comparaison pour qu'il soit plus pr√©cis et structur√© :

Prompt original : "{original_prompt}"

Techniques √† appliquer :
- Ajouter des crit√®res de comparaison sp√©cifiques (performances, co√ªt, facilit√© d'usage, etc.)
- Demander une analyse objective avec des exemples concrets
- Structurer la r√©ponse avec une introduction, analyse d√©taill√©e et conclusion
- Limiter la r√©ponse √† 300 mots maximum
- Demander un tableau comparatif si pertinent

Retourne UNIQUEMENT le prompt am√©lior√©, sans balises ni formatage sp√©cial.""",

        "analysis": f"""Am√©liore ce prompt d'analyse pour qu'il soit plus m√©thodique et complet :

Prompt original : "{original_prompt}"

Techniques √† appliquer :
- D√©finir clairement ce qui doit √™tre analys√©
- Demander une approche √©tape par √©tape
- Inclure des facteurs critiques √† consid√©rer
- Demander des implications et recommandations
- Structurer la r√©ponse de mani√®re logique
- Limiter √† 300 mots maximum

Retourne UNIQUEMENT le prompt am√©lior√©, sans balises ni formatage sp√©cial.""",

        "creation": f"""Am√©liore ce prompt de cr√©ation pour qu'il soit plus cr√©atif et structur√© :

Prompt original : "{original_prompt}"

Techniques √† appliquer :
- D√©finir pr√©cis√©ment ce qui doit √™tre cr√©√©
- Demander une approche cr√©ative et originale
- Inclure des √©l√©ments d'inspiration et de style
- Structurer le processus de cr√©ation
- Adapter le contenu au public cible
- Limiter √† 300 mots maximum

Retourne UNIQUEMENT le prompt am√©lior√©, sans balises ni formatage sp√©cial.""",

        "research": f"""Am√©liore ce prompt de recherche pour qu'il soit plus m√©thodique et pr√©cis :

Prompt original : "{original_prompt}"

Techniques √† appliquer :
- D√©finir clairement l'objet de la recherche
- Sp√©cifier les types de sources √† consulter
- Demander une synth√®se organis√©e et factuelle
- Inclure des crit√®res de fiabilit√©
- Structurer la pr√©sentation des r√©sultats
- Limiter √† 300 mots maximum

Retourne UNIQUEMENT le prompt am√©lior√©, sans balises ni formatage sp√©cial.""",

        "calculation": f"""Am√©liore ce prompt de calcul pour qu'il soit plus p√©dagogique et m√©thodique :

Prompt original : "{original_prompt}"

Techniques √† appliquer :
- D√©finir clairement le probl√®me √† r√©soudre
- Demander une approche √©tape par √©tape
- Inclure des explications p√©dagogiques
- Montrer tous les calculs de mani√®re d√©taill√©e
- V√©rifier la coh√©rence des r√©sultats
- Limiter √† 300 mots maximum

Retourne UNIQUEMENT le prompt am√©lior√©, sans balises ni formatage sp√©cial.""",

        "general": f"""Am√©liore ce prompt g√©n√©ral pour qu'il soit plus clair et structur√© :

Prompt original : "{original_prompt}"

Techniques √† appliquer :
- Clarifier l'objectif de la demande
- Structurer la r√©ponse de mani√®re logique
- Adapter le niveau de d√©tail au contexte
- Utiliser un langage clair et accessible
- Organiser l'information de mani√®re professionnelle
- Limiter √† 300 mots maximum

Retourne UNIQUEMENT le prompt am√©lior√©, sans balises ni formatage sp√©cial."""
    }
    
    return templates.get(prompt_type, templates["general"])

def apply_manual_improvement(original_prompt: str, prompt_type: str) -> str:
    """Applique une am√©lioration manuelle simple si l'IA ne transforme pas assez le prompt"""
    
    # Am√©liorations simples et directes selon le type
    improvements = {
        "comparison": f"Fais-moi un tableau comparatif d√©taill√© de {original_prompt}, en analysant les avantages, inconv√©nients, cas d'usage et recommandations pour un contexte de gestion de projet.",
        
        "analysis": f"Analyse en d√©tail {original_prompt} en tant qu'AMOA, en expliquant les enjeux, les risques, les parties prenantes et les solutions possibles avec des exemples concrets de projets.",
        
        "creation": f"Cr√©e {original_prompt} de mani√®re structur√©e et professionnelle, en adaptant le style au contexte de gestion de projet et en incluant les bonnes pratiques m√©tier.",
        
        "research": f"Fais-moi une synth√®se des meilleures pratiques et m√©thodologies sur {original_prompt}, avec des sources fiables et une pr√©sentation adapt√©e aux professionnels de la gestion de projet.",
        
        "calculation": f"Calcule {original_prompt} en montrant tous les calculs √©tape par √©tape, les m√©triques de projet et en expliquant chaque m√©thode utilis√©e dans un contexte de gestion de projet.",
        
        "general": f"Explique {original_prompt} de mani√®re professionnelle et accessible, comme si tu parlais √† un chef de projet ou un AMOA qui a besoin de conseils pratiques."
    }
    
    return improvements.get(prompt_type, improvements["general"])

@app.get("/health")
async def health_check():
    """Endpoint de sant√© du service"""
    return {"status": "healthy", "service": "IA'ctualit√©s Real Backend"}

@app.post("/query", response_model=QueryResponse)
async def query_model(request: QueryRequest):
    """Endpoint principal pour interroger un mod√®le LLM"""
    start_time = time.time()
    
    try:
        # Routage vers l'API appropri√©e
        if request.model in BEDROCK_MODELS:
            result = query_bedrock_model(request.model, request.prompt)
        elif request.model in AZURE_MODELS:
            result = query_azure_model(request.model, request.prompt)
        else:
            available_models = list(BEDROCK_MODELS.keys()) + list(AZURE_MODELS.keys())
            raise HTTPException(
                status_code=400, 
                detail=f"Mod√®le '{request.model}' non support√©. Mod√®les disponibles: {available_models}"
            )
        
        # Ajouter le temps de traitement
        processing_time = round(time.time() - start_time, 2)
        result["processing_time"] = processing_time
        
        return QueryResponse(**result)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur interne: {str(e)}")

@app.post("/improve-prompt")
async def improve_prompt(request: dict):
    """Endpoint pour am√©liorer un prompt avec les meilleures pratiques de prompt engineering"""
    try:
        original_prompt = request.get("prompt", "")
        
        if not original_prompt.strip():
            return {"improved_prompt": "Veuillez saisir un prompt √† am√©liorer."}
        
        print(f"üîß Am√©lioration du prompt: '{original_prompt}'")
        
        # Analyse du type de prompt et application des meilleures pratiques
        prompt_type = analyze_prompt_type(original_prompt)
        print(f"üìä Type d√©tect√©: {prompt_type}")
        
        # Instructions simples et directes pour l'am√©lioration
        optimization_prompt = f"""Tu es un expert en prompt engineering. Am√©liore ce prompt pour qu'il soit plus pr√©cis et efficace.

PROMPT ORIGINAL : "{original_prompt}"
TYPE D√âTECT√â : {prompt_type}

INSTRUCTIONS :
- Transforme le prompt original en quelque chose de plus pr√©cis et efficace
- Ajoute des d√©tails sp√©cifiques selon le type de prompt
- Rends le prompt plus clair et structur√©
- Demande une r√©ponse bien format√©e avec des titres, sous-titres et listes
- Ne commence pas par "Voici le prompt am√©lior√©" ou des phrases similaires

EXEMPLES DE TRANSFORMATION :
- "Comment structurer un projet de transformation digitale" ‚Üí "En tant qu'AMOA, guide-moi √©tape par √©tape pour structurer un projet de transformation digitale. Structure ta r√©ponse avec des titres clairs (## Phase 1, ## Phase 2, etc.), des listes √† puces pour les √©tapes, et des sections d√©taill√©es pour la m√©thodologie, les parties prenantes, les risques et le planning recommand√©."
- "Comparer les m√©thodologies agiles" ‚Üí "Fais-moi un tableau comparatif d√©taill√© entre Scrum, Kanban et SAFe. Structure ta r√©ponse avec des titres (## Comparaison, ## Recommandations), des listes pour les avantages/inconv√©nients, et des sections claires pour chaque m√©thodologie."

Retourne UNIQUEMENT le prompt am√©lior√©, sans introduction ni explications."""

        print(f"üìù Prompt d'optimisation envoy√© √† l'IA")

        # Utiliser Claude 3 Sonnet pour l'am√©lioration
        try:
            result = query_bedrock_model("Claude 3 Sonnet", optimization_prompt)
            raw_response = result["response"].strip()
            
            # Nettoyer la r√©ponse pour extraire le prompt am√©lior√©
            if "Voici le prompt am√©lior√©" in raw_response:
                # Extraire le contenu apr√®s les marqueurs
                lines = raw_response.split('\n')
                improved_lines = []
                capture = False
                
                for line in lines:
                    if any(marker in line for marker in ["[CONTEXTE]", "<CONTEXTE>", "```"]):
                        capture = True
                    if capture:
                        improved_lines.append(line)
                
                improved_prompt = '\n'.join(improved_lines).strip()
                
                # Nettoyer les marqueurs de code
                if improved_prompt.startswith('```'):
                    improved_prompt = improved_prompt[3:]
                if improved_prompt.endswith('```'):
                    improved_prompt = improved_prompt[:-3]
                improved_prompt = improved_prompt.strip()
                
            else:
                # Si pas de formatage sp√©cial, utiliser directement
                improved_prompt = raw_response
            
            print(f"‚úÖ R√©ponse Claude 3 Sonnet trait√©e: {improved_prompt[:100]}...")
            
        except Exception as bedrock_error:
            print(f"‚ùå Erreur Bedrock: {bedrock_error}")
            # Fallback vers l'am√©lioration manuelle
            improved_prompt = apply_manual_improvement(original_prompt, prompt_type)
        
        # Validation : s'assurer que le prompt am√©lior√© est diff√©rent de l'original
        if improved_prompt.lower() == original_prompt.lower() or len(improved_prompt) < len(original_prompt) + 10:
            print(f"‚ö†Ô∏è Prompt pas assez diff√©rent, application de l'am√©lioration manuelle")
            improved_prompt = apply_manual_improvement(original_prompt, prompt_type)
        
        print(f"üéØ Prompt am√©lior√© final: {improved_prompt[:100]}...")
        return {"improved_prompt": improved_prompt}
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©rale: {e}")
        # Si l'am√©lioration √©choue, retourne le prompt original
        return {"improved_prompt": request.get("prompt", "")}

@app.get("/")
async def root():
    """Page d'accueil du backend r√©el"""
    return {
        "message": "ü§ñ IA'ctualit√©s Real Backend",
        "description": "Backend avec vraies API AWS Bedrock + Azure OpenAI",
        "version": "1.0.0",
        "models": {
            "bedrock": list(BEDROCK_MODELS.keys()),
            "azure": list(AZURE_MODELS.keys())
        },
        "endpoints": {
            "health": "/health",
            "query": "/query", 
            "improve_prompt": "/improve-prompt"
        },
        "frontend_url": "http://localhost:3000"
    }

if __name__ == "__main__":
    import uvicorn
    print("üöÄ D√©marrage du backend R√âEL IA'ctualit√©s...")
    print("üîë Utilisation des vraies API AWS Bedrock + Azure OpenAI")
    print("üì± Frontend React : http://localhost:3000")
    print("üîå Backend API : http://localhost:8000")
    print("üìö Documentation : http://localhost:8000/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True) 